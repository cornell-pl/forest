%% \section{Implementation}\label{sec:imp}

%% This section describes a number of refinements and optimizations
%% to our basic algorithms.

%% \subsection{Token families}
%% So far, parsing a \cd{Sync} token yields
%% one of three results: \cd{Good}, \cd{Fail} or \cd{Recovered}. 
%% In the actual implementation, a \cd{Sync} token can be not only a constant string, but also
%% a constant integer, an integer range or a combination thereof.
%% Consider parsing the token \cd{Sync (Str "GET")} when
%% the current input starts with ``POST.'' The
%% \cd{parse\_base} function indicates the result should be \cd{Fail}.
%% In reality, the input ``POST'' is in the same {\em family} as ``GET,'' 
%% \ie{}, a word,
%% and it may very well be that this \cd{Sync} token should have been 
%% an enumeration of words rather than a single word.
%% To handle such cases, we created a fourth type of parse node, \cd{Partial}, 
%% to indicate that the input belongs to the same family as the expected
%% token but does not match exactly, \ie, it is {\em partially} correct.
%% During aggregation, partial nodes cause the description 
%% to be specialized to include the additional values.  In the above example, the aggregate 
%% function will change the description to \cd{Sync (Enum [Word "GET", Word "POST"])}.
%% Such partial nodes reduce the number of parsing errors
%% and produce more compact and meaningful descriptions.

%% \subsection{Deterministic parsing}
%% In section \ref{sec:parse}, we presented a non-deterministic parsing semantics
%% for unions and arrays. While this is a correct semantics, it is expensive
%% to implement and is different from the \pads{} semantics of these two 
%% types.
%% Therefore in the prototype system, parsing a union involves 
%% attempting to parse the first branch and,
%% if it has error, attempting the second branch. This type of union semantics
%% is known as {\em deterministic choice}, as opposed to a non-deterministic
%% choice where both branches are always attempted.
%% When parsing an array, we use {\em longest match} semantics,
%% which means the parsing of the array elements and separators
%% continues until no more progress can be made in
%% the input. We will not elaborate these semantics because of lack of space.

%% \subsection{Initial batch and incremental batches}
%% We mentioned in Section \ref{sec:algo}, that we can learn an initial
%% description from any $N$ records from the source. In the implementation,
%% we simply take the {\em first} $N$ records from the source. The potential
%% problem with this
%% is that the final description could be skewed toward the first part of the
%% data. We choose to do this nonetheless because we do not want to make 
%% the assumption that we have the whole data file before we start learning
%% the description.  Furthermore, instead of learning the error data and 
%% updating the description every $M$ records which is indicated in 
%% \figref{fig:inc-learning}, we defer this till we have seen $M$ bad records.
%%  This avoids invoking the \cd{update\_desc} function unnecessarily when
%% there is no error data.

\subsection{Optimizations}\label{sec:opt}
The pseudo-code in \figref{fig:inc-learning} suggests the number of
aggregates is of the order $O(m ^ n)$, where $m$ is the maximum number of
parses for a line of input  and $n$ is the number of lines to
aggregate.  Clearly, this algorithm will not scale 
unless $m$ and $n$ are bounded.  To deal with this problem,
we have implemented several optimizations to limit the number of 
parses and aggregates. 

One key optimization culls parses based on 
the parse metric \cd{m}. 
%Metric $m_1$ is better than $m_2$ if $m_1$ is perfect and $m_2$ is not, 
%or if $m_1 > m_2$.
To be more precise, we instrument the implementation of the \kw{parse} function 
to return a list of 
{\em parse triples} \cd{(r,m,j)}, where \cd{r} is the data representation of
the parse, \cd{m} is the metric associated with \cd{r}, and
\cd{j} is the position in the input after the parse rather than just representations.
We define a \kw{clean} function that first partitions the
triples into groups that share the same 
{\em span}, \ie{}, the substring of the input consumed by the parse.
For each group, \kw{clean} retains all perfect parses. If 
none exist, it retains the best $k$ non-perfect parses in the group. 
We justify discarding the other triples because
given a description \cd{D} and a fixed span, we always
prefer the parse with the best metric. This idea is
similar to the dynamic programming techniques used in 
Earley Parsers \cite{earley-parser}. 

To understand the impact that different values of $k$ can have,
consider $k=1$, which causes the system to discard all erroneous parses except
the one with the best parse metric. This strategy significantly reduces the
cost of the parsing and aggregation operations, but we have found
in practice that it usually results in sub-optimal overall parse structures. 
For example, suppose the description in \figref{fig:kcutoff}
is the current top-level description \cd{D} when learning 
the \ai{} format.
\begin{figure}
\begin{code}
\kw{Precord} \kw{Pstruct} entry_t \{
         Phostname      host;
   ' ';  
   '-';  /* the first dash */
   ' ';  
   '-';	 /* the second dash */   
   " ["; Pdate          date;
   ':';  Ptime          time;
   "] "; request_t      request;
   ' ';  Pint           response;
   ' ';  Pint           length;
\};
\end{code}
\caption{Example description \texttt{D}}
\label{fig:kcutoff}
\vskip -2ex
\end{figure}
Recall that constants in descriptions, such as
\cd{' '} and \cd{'-'}, are used as sync tokens during
parsing. Now, suppose  we use \cd{D} to parse the second record 
in \figref{fig:ai}.
\cd{D} parses the data without error until it
reaches the second dash in \cd{D}. The description then expects a sync token
'-', but instead sees a string \cd{amnesty}.
According to the semantics of sync tokens, given in \secref{sec:parse-sem},
there can be two possible parses:

\noindent
{\small
\verb#(Recovered "kim [10/May/2009:18:38:35 ", (1, 26, 1))#
}
\noindent
or:\\
\noindent
{\small
\verb#(Fail, (1, 0, 0))#
}

In the first parse, the parser skips 26 characters before finding a \cd{'-'} token.
In the second, the parser returns failure and consumes no characters
from the input.
According to our ranking function on parse metrics, 
the first parse is better and therefore the
second one is discarded. Unfortunately, this strategy causes the parsing of the
remaining types up to \verb#"] "# to fail, thereby giving a bad overall parse.
If we keep the second parse and continue with it, the next sync token 
\verb#" ["# will be able to recover after skipping \cd{kim} and succeed
in parsing the rest of the types, hence giving a better overall parse.
Therefore, in the implementation, we have chosen $k$ to be small 
(for speed) but larger than one (for quality).

A second optimization, the {\em parse cut-off} optimization, terminates a
candidate parse when parsing a struct with multiple
fields $f_1$, $f_2$, ..., $f_n$ if the algorithm encounters 
a threshold number of errors in succession. 
This may result in no possible parses for the
top-level description, in which case we restart the process
with this optimization turned off. 

A third optimization is memoization.
The program keeps a global memo table indexed by the pair of a
description \cd{D} and the beginning position for parsing \cd{D}.  This table
stores the result for parsing according to \cd{D} at the specific position.
%% Finally, we bound the total number of aggregates the
%% algorithm can produce by selecting the top
%% $k$ aggregates with the fewest number of \cd{Opt} and \cd{Learn}
%% nodes. 


