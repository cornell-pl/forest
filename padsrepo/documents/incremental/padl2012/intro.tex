\section{Introduction}
%\begin{figure*}
%Cosmos event streams:
%{\small
%\begin{verbatim}
%EventName=CosmosPerfCounter_1,CurrentTimeStamp=2010-04-10T02:07:54.950Z,Machine=msradb001,
%name=\\Cosmos\\MessageSent,instanceName=Total,counterType=rate,currentValue=1,minValue=1,
%maxValue=1,average=0,rate=1.18217E-006,runningTotalCount=0,runningTotal=0,timeIntervalMs=
%845901093,CsProcessId=NONE
%\end{verbatim}
%}
%Messages.sdb:
%{\small
%\begin{verbatim}
%Jun  4 10:42:56 nid00004 sshd[5405]: Accepted publickey for root from 193.168.0.1 port 43484 ssh2
%Jun  4 10:57:49 nid00003 syslog-ng[3504]: syslog-ng version 1.6.8 starting
%Jun  4 10:57:53 nid00003 sshd[4069]: Server listening on :: port 22.
%\end{verbatim}
%}
%\caption{Example ad hoc data sources}\label{fig:adhoc}
%\vskip -2ex
%\end{figure*}


Ad hoc data is any {\em non-standard}, {\em semi-structured} 
data source for which processing tools and libraries are not
readily available. HTML, XML, and data in relational databases are not ad hoc because 
many tools exist to manage such data.
Despite efforts to standardize data formats, ad hoc data persists
in many domains ranging from computer system administration to 
financial transactions
 to health care to computational biology. Figure \ref{fig:ai} shows
an example of a piece of ad hoc data source.

\begin{figure*}
{\scriptsize
\begin{verbatim}
207.136.97.49  - - [05/May/2009:16:37:20 -0400] "GET /README.txt HTTP/1.1" 404 216
ks38.kms.com - kim [10/May/2009:18:38:35 -0400] "GET /doc/prev.gif HTTP/1.1" 304 576
\end{verbatim}
}
\caption{A Fragment of a Simple Web Server Log \ai{}}
\label{fig:ai}
\end{figure*}

%Figure \ref{fig:adhoc} shows fragments of two ad hoc data sources.
%The first example comes from Microsoft's distributed computing
%infrastructure Cosmos. The second example is excerpted from the file 
%\texttt{/var/log/messages}
%on a CRAY supercomputer.

People continue to produce and use ad hoc data because such formats 
are expedient and compact.  
Typical uses of these data sources include system fault monitoring
by tracking vital system health parameters in the system logs,
intrusion detection by matching access patterns to intrusion
models and data mining of scientific and financial data.

Despite the expediency of producing ad hoc data, these
data formats become very difficult to deal with because of missing
documentation, the lack of tools, and corruptions caused by
repeated redesign and re-engineering over time. 
In the past, ad hoc data analysis usually involved
writing a shell script or one-off wrapper program to parse each 
data format, a practice which is expensive, error-prone and brittle.

The \pads{} project \cite{padsweb} aims to solve the above 
problems. The central technology is a declarative, type-based, 
data description language that allows the user to specify the physical
layout of data sources as well as semantic properties of the data. 
\pads{} specifications can be compiled into a suite of
processing tools such as a statistical reporting tool, an 
XML converter and a query engine, and programming libraries 
including parser, printer and traversal functions. Figure \ref{fig:ai.p}
shows the \pads{} description for the \ai{} data source, and
Figure \ref{fig:xml} demonstrates the XML translator
output automatically generated from the \pads{} description.

\begin{figure}[t]
{\scriptsize
\begin{code}
\kw{Punion} client_t \{
  Pip       ip;      // 207.136.97.49
  Phostname host;    // ks38.kms.com
\};
\kw{Punion} auth_id_t \{
  Pchar unauthorized : unauthorized == '-';
  Pstring(:' ':) id;
\};
\kw{Pstruct} request_t \{
   "GET ";    Ppath    path;
   " HTTP/";  Pfloat   http_ver;
   '"';
\};
\kw{Precord} \kw{Pstruct} entry_t \{
         client_t       client;
   ' ';  auth_id_t      remoteID;
   ' ';  auth_id_t      auth;
   " ["; Pdate          date;
   ':';  Ptime          time;
   "] "; request_t      request;
   ' ';  Pint           response;
   ' ';  Pint           length;
\};
\end{code}
\vskip -2ex
}
\caption{\padsc{} description for the \ai{} format}
\label{fig:ai.p}
\end{figure}

\begin{figure}[t]
{\small
\begin{verbatim}
<entry_t>
  <client>
    <ip>
      <elt><val>207</val></elt>
      <elt><val>136</val></elt>
      <elt><val>97</val></elt>
      <elt><val>49</val></elt>
      <length>4</length>
    </ip>
  </client>
  <remoteID>
    <unauthorized><val>-</val></unauthorized>
  </remoteID>
  <auth>
    <unauthorized><val>-</val></unauthorized>
  </auth>
  <date><val>2009-05-05</val></date>
  <time><val>16:37:20</val></time>
  <timezone><val>-0400</val></timezone>
  <request> ...  </request>
  <response> ... </response>
  <length> ... </length>
</entry_t>
\end{verbatim}
}
\caption{XML translator output from one record of \ai{} format}\label{fig:xml}
\vskip -2ex
\end{figure}


The large scale as well as the streaming and evolving nature of many ad hoc
sources led us to believe that a system which automatically {\em learns}
a \pads{} description of a given data source and incrementally updates that
description as the source evolves could significantly improve the productivity of ad hoc data users.
As a first step, we developed an unsupervised algorithm \learnpads{}
\cite{Fisher+:dirttoshovels,fisher+:sigmod08}
that automatically infers a \pads{} description of a data source by 
computing frequency statistics for the {\em tokens} in the data and using an information
theoretic score to guide description optimization. 

This algorithm, however, has three important limitations:
first, it requires that all data fit into main memory and contains procedures 
that are quadratic to the size of data, 
and therefore cannot {\em scale} to very large sources; 
second, when the data format evolves over time, 
the description has to be learned from scratch; 
and finally, machine learned description, 
while optimized for both precision and conciseness at the same time, 
may not be very user-friendly in terms of readability.  

In this paper, we propose a new algorithm that {\em incrementally} infers 
descriptions of large scale or evolving ad hoc data sources. 
The system takes as input an initial description 
and a new batch of data. It returns a
modified description that extends the initial description and covers the new 
data. The initial description may be supplied by the user or automatically
generated using the original \learnpads{} system. This iterative architecture
enables the learning of a very large data source by partitioning it 
into smaller batches and updating the description from one batch to the next. 
It also allows the user to modify the description output at the end of
an iteration (\eg{}, renaming the automatically generated variable names), 
and insert the revised description back into the loop.

The main contributions of this paper are:
\begin{enumerate}
\item The design of a new system for generation of data descriptions
and end-to-end ad hoc data processing tools from example data.  
The system is incremental and interactive, allowing it to process
streaming data a chunk at a time, and allowing users to intercede
to correct, adapt or modify intermediate results. 
\item The engineering and optimization of algorithms that allow
the system to handle large, industrial data sources of 30GB or
more in a matter of a few hours.
\item The evaluation and analysis of the system
on 16 different examples drawn from
various industrial data sources.
\end{enumerate}

%We presented a preliminary, 6-page abstract of this material in an informal workshop~\cite{wasl09:zhu+}
%that was later reprinted unchanged and unrefereed in a SIG newsletter. 
%Unlike the earlier paper, this paper presents a re-engineered algorithm
%that is capable of processing data sources more than two orders of magnitude
%larger, while at the same time producing outputs of higher quality. 
%This paper also gives detailed empirical analysis of the 
%correctness and scalability of the system.

In the rest of the paper, 
%we give a brief overview of \pads{} and the original
%\learnpads{} inference algorithm (in Appendix \secref{sec:review}). 
we describe the new incremental inference algorithm (\secref{sec:algo})
and give a comprehensive experimental evaluation of the system 
(\secref{sec:eval}). We then compare this
system with some related work (\secref{sec:related}) and finally conclude
the paper (\secref{sec:conclude}).
