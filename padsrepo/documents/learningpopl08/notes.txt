@inproceedings{1164139,
 author = {Geert Jan Bex and Frank Neven and Thomas Schwentick and Karl Tuyls},
 title = {Inference of concise DTDs from XML data},
 booktitle = {VLDB '06: Proceedings of the 32nd international conference on Very large data bases},
 year = {2006},
 pages = {115--126},
 location = {Seoul, Korea},
 publisher = {VLDB Endowment},
 }

[meta-note: even though this is not a directly related paper, it does seem
pretty cool and merits a deeper read than I gave it.  There may be ideas
in here that we could exploit at some point.]

This paper describes how to infer a DTD from raw XML data.
Some of the elements of the problem are similar to our problem.
In particular, the scheme is inferred without any intervention
from positive examples only.  However, other elements are quite different:

-- the authors are working with well-structured XML data as opposed to
ad hoc data. Ad hoc data has tokenization problems and 
is not a well-structured, tree-shaped collection of tags.

-- We evaluate the effectiveness of our techniques on ad hoc data.
It is impossible to know how the core ideas in this xml inference
algorithm might work on ad hoc data.

-- this paper shows how to infer a DTD, but does not show how to use it
to automatically generate end-to-end tools (accumulator, grapher, query
engine, xml-transformer).

-- a DTD can be approximated as context-free grammar where the right-hand
sides are regular expressions.  Consequently, in this context, the core 
problem solved by the authors is an inference mechanism for two subclasses
of regular expressions:
  -- SOREs -- where every atomic element can occur at most once in the 
                regular expression
  -- CHAREs -- where every regular expression is a sequence of "factors"

-- even omitting constraints, dependencies and switches, the sorts of
grammars that we infer are not restricted to SOREs or CHAREs.  It
would be interesting to investigate what would happen if we try to
make such a restriction or whether it's simply impossible 
to come up with any reasonable description in such cases.

-- the algorithms used in our paper are completely different

=========================================

@inproceedings{DBLP:conf/vldb/BexNV07,
  author    = {Geert Jan Bex and
               Frank Neven and
               Stijn Vansummeren},
  title     = {Inferring XML Schema Definitions from XML Data},
  booktitle = {VLDB},
  year                  = {2007},
  pages     = {998-1009},
  ee                = {http://www.vldb.org/conf/2007/papers/research/p998-bex.pdf},
  crossref  = {DBLP:conf/vldb/2007},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

-- this paper builds upon the paper above.  in the previous paper,
the DTD definition could not depend upon the parent node.  In this work,
the authors infer "k-local" SOREs where the definition of the SORE can
depend upon its parent, grant-parent, great-grand-parent, etc. to a maximum of
k levels up for some fixed constant k.  This allows the authors to infer
the more powerful XML Schema Definitions (as opposed to just DTDs) for 
their data.

-- these k-local SOREs include an element of dependency -- dependency on 
parents and grandparents.  Since ad hoc data doesn't have the same kind of
nesting structure as XML, we tend to find dependencies between siblings
and represent those as "switches".

-- most of the comparison with the previous paper also applies...

=======================================

InstanceToSchema tool
http://www.xmloperator.net/i2s/

this is open source software written in Java and released under a 
BSD-style license.  It infers RELAX NG schema for xml. 
I couldn't find any papers describing the inference techniques
in any detail.  I didn't look too hard.  There were no obvious links 
from the web page.

=======================================

On schema Discovery
Miller, et el.
http://citeseer.ist.psu.edu/miller03schema.html

This paper uses a clustering algorithm based on attributes to have some
basic mining of constraints on database relations. The purpose of this
is to discovery new database schema for data with errors. This is obviously
quite different from what we are doing. No stats analysis is done here.
The clustering is based on some distance metric defined on the DB attributes.

========================================

@inproceedings{DBLP:conf/webdb/GubanovB06,
  author    = {Michael Gubanov and
               Philip A. Bernstein},
  title     = {Structural text search and comparison using automatically
               extracted schema},
  booktitle = {WebDB},
  year                  = {2006},
  ee                =
{http://db.ucsd.edu/webdb2006/camera-ready/paginated/01-161.pdf},
  crossref  = {DBLP:conf/webdb/2006},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

This paper extracts schema from unstructured text such as instruction
manuals through the use of natural language processing techniques. In 
particular, it separate the nouns and verbs from each sentence and
try to classify the subject or concept of the sentences by the nouns and
generate attributes of the schema with the verbs or actions used. The
counting of distinct words here seems analogous to the building of histograms
of tokens in our paper, but the counting here is to decide the main
concept of the sentence and not the structure of the sentence. The 
structure is assumed to be fixed English grammar. And beyond this, the
techniques used in two papers are very different.

One thing to note is that many of the natural language processing techniques
require some prior knowledge of the domain (in this case nouns and verbs),
and the tokenization used in our work also require such knowledge (e.g.
date, time, url formats). In this sense, there is some similarity between
our work and NLP.

=================================
[Rus & Subramanian 97]
theoretical characterization of information capture and access
formalize the notion of a document segmenter, identifying
possibly-relevant fragments of the document, which are then examined
by structure detectors, which look for pattersn among the segments.
high-level indexing to help with IR queries

@article{239048,
 author = {Daniela Rus and Devika Subramanian},
 title = {Customizing information capture and access},
 journal = {ACM Trans. Inf. Syst.},
 volume = {15},
 number = {1},
 year = {1997},
 issn = {1046-8188},
 pages = {67--101},
 doi = {http://doi.acm.org/10.1145/239041.239048},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

================================

*** Reconciling schemas of disparate data sources: A machine-learning approach
Doan et al.
http://portal.acm.org/citation.cfm?doid=375663.375731

This paper introduces a system called LSD that learns the semantic mapping 
between between two DB schemas. The input to the learning system is already 
structured data such as XMLs and DTDs. We are learning from semi-structured
or unstructured data and we are not trying to identity semantic relations so
the problems are quite different. 

source data is assumed to be in XML
map raw data to "mediated schema"
mediated and source schemas represented as DTDs
user maps some raw daw data to source schema, system then infers how to generalize


==============================
"Learning information extraction rules for semi-structured and free text "
Soderland
http://www.cs.washington.edu/homes/soderlan/soderland_ml99.pdf  

This paper again describes a system called WHISK that learns information
extraction rules from natural language text. The text can be unstructured,
semi-structured or free text, such as ads from Graigslist. The rules 
learned are in a form of regular expression. The learning starts with 
a number of hand-tagged training set and some user-defined semantic classes
such as what words means bedroom, and some empty rules. Learning and 
human tagging happen interleavingly in iterations. The empty rules contain
slots which need to be filled in with semantic terms such as Time, Date,
Neighborhood, City, Bedroom. The slots are the properties of interest for
later extraction and the number of slots are fixed a priori. Most of the
comparison between natural language processing and our paper applies here.
The technique we are advocating is completely automatic, push-button kind,
whereas in this paper, human knowledge is require in the training (partly
because this is a very hard problem). The learning of regular 
expression could be a userful thing for us to look
deeper, for discovering new tokens in tokenization.

This paper is targeted at information extraction, rather than at
learning a description that covers the totality of the data.  It
requires users to tag the data and select the pieces of information
that they want to extract. It requires the extracted data form a
tuple, so it can't produce structured data as output.

must choose a priori the things to extract

It contains a very thorough descrition of related work on information
extraction that we could refer to as an overview of this kind of work.

=================================
@Article{DenisLemayTerlutte2004,
   author="F. Denis and A. Lemay and A. Terlutte",
   title="Learning regular languages using RFSAs",
   journal=TCS,
   year="2004",
   volume="313",
   number="2",
   pages="267-294"
}

This paper proposes an algorithm DeLeTe2 to learn a subset of 
regular languages called residual languages by identifying
inclusion relations among them as opposed to equivalence, from
only positive samples. The result of that is a residual finite 
state automaton (RFSA) instead of a DFA. Because learning 
RFSA is not possible in polynomial time, the paper attempts to 
learn a language that is larger than minimum instead. 
Comparison of learning automata with our work above also
applies here.

==================================
@InProceedings{fernau02a,
   booktitle="Proceedings International Workshop on Machine Learning and Data
Mining in Pattern Recognition (MLDM 2001)",
   author="H. Fernau",
   title="Learning XML grammars",
   publisher=SV,
   series=LNCS,
   volume=2123,
   pages="73-87",
   year=2001
}

This paper is an earlier attempt than the Bex et al. above to infer
DTDs from XMLs. They propose a generalized framework of learning
a sub-class of regular language (such as XML) by limiting the language
with a distinguishing function. The algorithm proposed is exponential to
the size of this function. They claim to generalize over k-reversible
language by Angluin and the terminal distinguishing languages by 
Radhakrishnan. I will have to read those two papers to tell more of
the story. This paper proved some properties about XML and also 
the biased language that they infer. For comparison between this paper
and our work, similar argument in the comments on Bex et al. applies
here. Also notice that our method is more of a heuristic nature.  

====================================
@InProceedings{RaeymaekersBruynoogheVandenBussche05,
   series=LNAI,
   year="2005",
   booktitle="Proceedings of ECML'2005" ,
   pages="305--316",
   volume="3720",
   author="Stefan Raeymaekers and Maurice Bruynooghe and  Jan {Van den
Bussche}",
   title="Learning (k,l)-Contextual Tree Languages for Information Extraction"
}

This paper is another attempt at learning a wrapper based on a tree
automaton that represents HTML/XML data. This tree language limits the height
and the cardinality of the input tree to l and k respective to achieve a
balanc between expressiveness and generality. The value l and k need to be
tune with both positive and negative examples. The paper claims to require
fewer positive and negative examples to learn the grammar compared to
previous attempts.

======================================
[2006] Aur√©lien Lemay and Joachim Niehren and R√©mi Gilleron, Learning n-ary
Node Selecting Tree Transducers from Completely Annotated Examples
<http://hal.ccsd.cnrs.fr/view_by_stamp.php?label=INRIA&langue=en&action_todo=vi
ew&id=inria-00088077&version=1>, /International Colloquium on Grammatical
Inference/, Lecture Notes in Artificial Intelligence *4201*, 253-267

This paper looks at the problem of learning queries from XML or HTML trees. 
They propose a polynomial time algorithm to learn tree automata that represent
node queries in the XML trees. The algorithm trains on both positive and 
negative examples. And the examples have to be annotated by human. 
This problem is quite different than our problem which is ad hoc
data and not in any tree structure to begin with. Our learning systems also only
trains on positive examples. 

========================================
@InProceedings{MusleaMintonKnoblock03,
   booktitle="IJCAI 2003",
   year="2003",
   title= "Active learning with strong and weak views: a case study on wrapper
induction",
   pages="415--420",
   author="Ion Muslea and Steve Minton and Craig Knoblock"
}

This paper presents a technique to select document samples for human to label
in "active learning". The technique is called aggressive co-testing
which is an improvement from their earlier Naive Co-testing method
which uses two strong views (one forward and one backward) on sample
data. A view in this context is a set of features from the document
that is sufficient to learn the exact concept. Co-testing is applicable
to documents which has multiple disjoint view, each of which sufficient
for learning. Instead of randomly choose examples to label, the
Co-testing method pick those example on which the multiple views disagree,
that is, the concept learned from one view is different from another. 
This method reduces the number of samples that require labeling effective.
The Agressive Co-Testing uses a weak view that doesn't give the exact
concept (but either more general or more specific) when learned on, in
addition to two strong views used in Naive Co-testing. 
The difference between this work and our work is, first this is a technique
that helps labeling samples, which is not even relevant to us; second
the technique is used for learning features in a document and not the structure
or grammar of the entire document like what we are doing; and
finally probability theory is not used here with these techniques. 

==========================================
@article{608666,
 author = {Ion Muslea and Steven Minton and Craig A. Knoblock},
 title = {Hierarchical Wrapper Induction for Semistructured Information Sources},
 journal = {Autonomous Agents and Multi-Agent Systems},
 volume = {4},
 number = {1-2},
 year = {2001},
 issn = {1387-2532},
 pages = {93--114},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 }
STALKER system
wrapper induction paper from web pages
user-labeled training set
can extract tree-structured data, 
can handle missing values
uses training data to find surrounding tags
==========================================
@inproceedings{DBLP
<http://www.informatik.uni-trier.de/%7Eley/db/about/bibtex.html>:conf/icml/Ires 
onCCFKL05,
 author    = {Neil Ireson and
              Fabio Ciravegna and
              Mary Elaine Califf and
              Dayne Freitag and
              Nicholas Kushmerick and
              Alberto Lavelli},
 title           = {Evaluating machine learning for information extraction},
 booktitle = {ICML},
 year           = {2005},
 pages           = {345-352},
 ee           = {http://doi.acm.org/10.1145/1102351.1102395},
 crossref  = {DBLP:conf/icml/2005},
 bibsource = {DBLP, http://dblp.uni-trier.de}
}

This paper documents an information extraction contest which uses uniform
comparison among all the participants. The competition asks the participants
to learn important features from a number of annotated CFP documents, and then
cross validate on a larger number of test documents. Altogether 18 
different systems were used in the competition. The paper compared the accuracy
and learning curves of these systems. Conclusions include that different
systems implementing the same ML algorithm can perform very differently and
many systems exhibit over-fitting. Parameterization plays a critical role sometimes
in the performance of the algorithm. This paper is not directly related to the kind of
techniques we introduce in our work. However, the empirical methodology is interesting
and could help us in presenting our data to the ML community.

========================================================

Xtract: Learning Document Type Descritors from XML Document Collections
Garofalskis et al.

This is one of the influential papers on XML DTD inferencing. The basic
idea is to generate one regular expression for each node (element name) in
the XML, where the symbols in the expression is the sub-element names under
that node. So at a node level, the DTD inference problem is exactly the 
same as our problem after our data file is completely tokenized. 
The tokens are essentially the sub-element names. The paper takes a number of
positive examples, and starts with a regex which is the union of all
the positive examplees, and then used some generalization techniques to
create sequences (Kleene star) and tuples. The resulting grammar is
more general than the original grammar. Then it uses factoring to simplify the
grammar. During generalization, a number of possible candidate may result, 
MDL scores are computed for each candidate and a best is chosen in the end.
The generalization and factoring are essentially a set of rewriting rules that
leads to a search problem. This is similar to our approach. However, because
the initial grammar can be extremely complex, it is questionable how
effective these rewriting can be. 

==========================================
Wrapper Induction for Information Extraction
Nicholas Kushmerick, IJCAI 1997

This paper (as well as the PhD thesis of the same title) proposed a framework for
extract information from web pages. A wrapper is a function that returns some 
tuples of interest from a web page. It is represented by a procedure such as
"skip everything till first occurrence of h, until an occurence of t, go to next element
between tag l_i and r_i and extract it as the next attribute in the tuple." 
The kind of wrapper being induced here is what they called HLRT wrapper which is
suitable for tabulated content. 

Given a set of sample pages, the framework first use an oracle function to label
the samples. The oracle function makes use of a collection of pre-defined
domain-specific reusable heuristics called the "recognizers" to identify things 
of interest e.g. what constitutes a phone number or what is a country name, etc. 
The recognizers might not be perfect and a "corroboration" process takes care of
the errors in recognization attributes. Labels such generated might be ambiguous.
This is handled by a probablistic model called PAC. The labelling technique is
the key contribution of this paper. 

The number of samples needed to be confident about the induction result is governed
by PAC model which defines an error metric over hypothesis. With each sample page
labeled, a BuiltHLRT algorithm runs through each page, and extract the common
header and footer as well as the prefixes and suffixes of each labeled attribute, and
construct the wrapper. 

The techniques introduced here only requires positive samples and the labelling 
process can be related to the tokenization process of our work. In fact, it will be
interesting to see if the corroboration algorithm can be applied somehow to
our tokenization. However there are significant differences between this work and
ours: 

1. The problem being solved here is information extraction and not learning of
the entire structure of the documents, so our goals are different;
2. The technique only works for data in tabular format with formatting
tags of some kind
3. the induction used here is really simple which basically constitutes of 
a conjunction of all the features each sample page pocesses and calls that 
a generalization. No stats analysis of any kind is used in the
induction.

extracted data is generally relational, although extends to
 tree-structured data
 missing values are not permitted
don't have to describe extraneous stuff
does it assume data is xml or html? 
data required to conform to one of 6 templates,
 ie, assumes the structure of the data, looks for delimiters
 labelled data allows system to find payload and the search
 surroundings for delimiters
recognizers are chosen by user?
 corroboration requires one recognizer to be 'perfect' and none to be unreliable
recognizer categories: perfect, incomplete (false negs), unsound
 (false +), unreliable (both false - and false +)
 in chap 6, ignore unreliable recognizers
chapter 6 uses only tabular recognizers
what if multiple attributes have the same "type"?
user provides input pages, ordering, and recognizer library

the pac model is very specific to templates that he uses

@phdthesis{kushmerick-phd1997,
  author = {N. Kushmerick},
  year = {1997},
  title = {Wrapper induction for information extraction},
  school = {University of Washington},
  note = {Department of Computer Science and Engineering},
  psgz = {http://kushmerick.org/nick/research/download/kushmerick-phd.ps.gz},
  topic = {information extraction; machine learning; data integration},
  }

=================================
Shopbot [Doorenbos et al 97, Perkowitz et al 97]

"A scalable comparison shopping agent for the world-wide web"
Agents-97, Feburayr 1997


@inproceedings{267666,
 author = {Robert B. Doorenbos and Oren Etzioni and Daniel S. Weld},
 title = {A scalable comparison-shopping agent for the World-Wide Web},
 booktitle = {AGENTS '97: Proceedings of the first international conference on Autonomous agents},
 year = {1997},
 isbn = {0-89791-877-0},
 pages = {39--48},
 location = {Marina del Rey, California, United States},
 doi = {http://doi.acm.org/10.1145/267658.267666},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }


Primitive version of our techniques, applied to html pages
divides pages into chunks based on newline html tags
non-html code abstracted with meta-tokens to produce a template, one
for each chunk
each template then measured to see what percentage of page explained
by template.  Templtes with high hit rate used to extract data

hard-coded to html tags
Looking for prices and product descritions
knows the name of various products


"standard grammar induction inappropriate because it requires a lot of
data and it must be supervised"

=================================
Ariadne [Ashish & Knoblock 97a, 97b]
specific to html
extrats data from  hierarchically structured apges
uses font size and html tag semantics to determine hierarchy
has fixed templates for page structure.  attempts to match that
structure

@inproceedings{276381,
 author = {Jos\'{e} Luis Ambite and Naveen Ashish and Greg Barish and Craig A. Knoblock and Steven Minton and Pragnesh J. Modi and Ion Muslea and Andrew Philpot and Sheila Tejada},
 title = {Ariadne: a system for constructing mediators for Internet sources},
 booktitle = {SIGMOD '98: Proceedings of the 1998 ACM SIGMOD international conference on Management of data},
 year = {1998},
 isbn = {0-89791-995-5},
 pages = {561--563},
 location = {Seattle, Washington, United States},
 doi = {http://doi.acm.org/10.1145/276304.276381},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }
=================================

@inproceedings{Lerman+:webtables,
 author = {Kristina Lerman and Lise Getoor and Steven Minton and Craig Knoblock},
 title = {Using the structure of Web sites for automatic segmentation of tables},
 booktitle = {SIGMOD '04: Proceedings of the 2004 ACM SIGMOD international conference on Management of data},
 year = {2004},
 isbn = {1-58113-859-8},
 pages = {119--130},
 location = {Paris, France},
 doi = {http://doi.acm.org/10.1145/1007568.1007584},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

automatic extraction from web pages
uses structure of web site containing pages to help identify relevant information
assumes data to extract is tabular
uses HMM techniques to find this data
=================================

@inproceedings{Pinto+:texttables,
 author = {David Pinto and Andrew McCallum and Xing Wei and W. Bruce Croft},
 title = {Table extraction using conditional random fields},
 booktitle = {SIGIR '03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval},
 year = {2003},
 isbn = {1-58113-646-3},
 pages = {235--242},
 location = {Toronto, Canada},
 doi = {http://doi.acm.org/10.1145/860435.860479},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

extracting tabular data from free-form text
and to identify payload rows from meta-data
uses user-labeled data to train
has features specific to tabular layouts
compares using conditional random variables (CRF)s with HMMs
=================================
[Douglas etal 95, Doughlas & Hurst 96]
techniques for identifying the tabular structure in plan text documents
=================================

[H.T.Ng, C.Y.Kim and J.L.T.Koo.  Learning to recognize tables in free text. 
Proc. of the 37th annual meeting of the assoc. for computational linguistics, pages 443-450, 2002
@inproceedings{Ng+:texttables,
 author = {Hwee Tou Ng and Chung Yong Lim and Jessica Li Teng Koo},
 title = {Learning to recognize tables in free text},
 booktitle = {Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics},
 year = {1999},
 isbn = {1-55860-609-3},
 pages = {443--450},
 location = {College Park, Maryland},
 doi = {http://dx.doi.org/10.3115/1034678.1034746},
 publisher = {Association for Computational Linguistics},
 address = {Morristown, NJ, USA},
 }

machine learning techniques to identify rows and colums in table, only interested in finding tables in text.

==========================================================

*** Automatic segmentation of text into structured records
Borkar et al.
http://portal.acm.org/citation.cfm?doid=375663.375682

This paper describes another natural language processing technique to
extract records from unstructured text such as addresses, bib entries,etc.
It learns a Hidden Markov Model which is a probablistic finite state
automata and a dictionary of elements from a set of training data. The HMM
describes the possible structure of the text e.g. an address is house
number followed by road and followed by city with certain probabilities.
And the dictionary tells which words are likely part of a street name, and
which words are likely to be a city, etc. The problem they are attacking is
quite different from ours. We are not trying to give semantics to
free text like they do.

Tool called: DataMold
seeded with training examples
training example is a pair of:
 1. a list of elements to be extracted
 2. labeled training data
does not assume an order to the elements, nor are all elements required to be present
learns a wrapper from training data to extract these elements

=================================

[14] P. Pyreddy and W. Croft. Tintin: A system for 
retrieval in text tables. In Proceedings of the Second 
International Conference on Digital Libraries, pages 
193–200, 1997. 

@inproceedings{263816,
 author = {Pallavi Pyreddy and W. Bruce Croft},
 title = {TINTIN: a system for retrieval in text tables},
 booktitle = {DL '97: Proceedings of the second ACM international conference on Digital libraries},
 year = {1997},
 isbn = {0-89791-868-1},
 pages = {193--200},
 location = {Philadelphia, Pennsylvania, United States},
 doi = {http://doi.acm.org/10.1145/263690.263816},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

uses character alignmet graphs (CAG) to find text tables in documents. the CAG abstracts table to text characters and spaces and attemps to identify titles, captions, and data rows.  No attempt to identify fine elements of table.

=================================
[13] D. Pinto, W. Croft, M. Branstein, R. Coleman, 
M. King, W. Li, and X. Wei. Quasm: A system for 
question answering using semi-structured data. In 
Proceedings of the JCDL 2002 Joint Conference on 
Digital Libraries, pages 46–55, 2002. 

@inproceedings{544228,
 author = {David Pinto and Michael Branstein and Ryan Coleman and W. Bruce Croft and Matthew King and Wei Li and Xing Wei},
 title = {QuASM: a system for question answering using semi-structured data},
 booktitle = {JCDL '02: Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries},
 year = {2002},
 isbn = {1-58113-513-0},
 pages = {46--55},
 location = {Portland, Oregon, USA},
 doi = {http://doi.acm.org/10.1145/544220.544228},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

builds on cag w/ heuristic method to extract individual cells for QA.  
some lang. proc to distinghish headers from data rows
ir paper
=================================
@inproceedings{Cresenzi+:roadrunner,
 author = {Valter Crescenzi and Giansalvatore Mecca and Paolo Merialdo},
 title = {RoadRunner: Towards Automatic Data Extraction from Large Web Sites},
 booktitle = {VLDB '01: Proceedings of the 27th International Conference on Very Large Data Bases},
 year = {2001},
 isbn = {1-55860-804-4},
 pages = {109--118},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
 }

roadrunner: finding templates from generated web pages

=================================

information extraction
 template extraction
 whisk : sodorburg
 kuf thesis
 table identification
